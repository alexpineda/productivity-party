# Project Instructions

Use this specification and these guidelines as you build the **Screenpipe Plugin**.

Write the complete code for every step—no shortcuts.

The goal is to fully implement whatever is requested.

You’ll see tags in the code. These are context tags that help you understand the codebase.

The screenpipe sdk and api documentation can be found in: `screenpipe_docs.md`

## Overview

This is a **Next.js plugin** for Screenpipe. It is embedded within a Node environment and uses:

- **Next.js** (App Router)
- **Tailwind**
- **Shadcn** (UI components)
- **Screenpipe** browser/server SDK (`@screenpipe/browser` and `@screenpipe/js`)
- OpenAI SDK for calling LLMs

## Project Structure

- `app` - Next.js app router
  - actions - NextJS Server Actions
  - `api` - API routes
    - `route.ts` - Example routes referencing `@screenpipe/js`
  - `(app)` app subgroup for routes
    - `page.ts` - the profile configuration is the home page
    - `chat` chat with other users via partykit
    - `leaderboard` see the productivity leaderboard
    - `productivity` see productivity data and configuration
    - `debug` debug helper for dev environment
  - `route` - An example route
    - `_components` - One-off components for the route
    - `layout.tsx` - Layout for the route
    - `page.tsx` - Page for the route
- `components` - Shared components
  - `ui` - UI components
  - `utilities` - Utility components
- `hooks` - Custom react hooks
- `lib` - Library code
  - `types` - Shared types
  - Other utility files
- `party-server` - Partykit server code
- `public` - Static assets
- `scripts` - Misc or build scripts
- `tsconfig.json`, `tailwind.config.ts`, `next.config.mjs`, etc.

## 1. General Rules

1. Use `@` to import from your local app code, unless you are importing from external packages (e.g., `@screenpipe/js`, `@screenpipe/browser`, `react`, etc.).
2. Use **kebab-case** for files and folders unless otherwise specified.
3. **Don’t update** Shadcn components unless explicitly told to.
4. We do not use our own environment variables. Any custom variable should be stored in user settings.

#### Type Rules

Follow these rules when working with types.

- When importing types, use `@/types`
- Name files like `example-types.ts`
- All types should go in `types`
- Make sure to export the types in `types/index.ts`
- Prefer interfaces over type aliases

An example of a type:

`types/actions-types.ts`

```ts
export type ActionState<T> =
  | { isSuccess: true; message: string; data: T }
  | { isSuccess: false; message: string; data?: never };
```

And exporting it:

`types/index.ts`

```ts
export * from "./actions-types";
```

## 2. Screenpipe-Specific Rules

#### Using the Screenpipe SDK

- **Client Components** should import from `@screenpipe/browser`.
- **Server Components** and **API routes** should import from `@screenpipe/js`. Prefer API routes over server components.
- You can also call the local REST endpoints (e.g., `http://localhost:3030/search`) directly if needed, but prefer the SDK methods.

#### Searching & Data Access

- Use `pipe.queryScreenpipe(...)` or `pipe.settings` as provided by `@screenpipe/js` or `@screenpipe/browser`.
- For advanced queries, use `POST /raw_sql` (still from the server side or API route).

#### Storing Plugin Data

- Use localstorage on client components. No plugin specific storage besides user settings and localstorage.

---

## 3. Relevant REST Endpoints

Here is a quick list of Screenpipe endpoints that are typically used in a plugin context. All routes default to `http://localhost:3030`.

6. **Search**

   - **`GET /search`**
   - **Purpose**: Find captured screen or audio content (OCR text, transcripts, UI data).
   - **Key Query Params**:
     - `q` (optional, single word)
     - `content_type` (`ocr`, `audio`, `ui`, or combos like `ocr+audio`)
     - `start_time`, `end_time` (ISO timestamps)
     - `app_name`, `window_name`, `speaker_ids`, etc.
   - **Example**:
     ```bash
     GET /search?q=meeting&content_type=ocr&limit=10
     ```

7. **Tags**

   - **`POST /tags/:content_type/:id`** (add tags), **`DELETE /tags/:content_type/:id`** (remove tags)
   - **Purpose**: Assign or remove tags for frames (vision) or audio chunks.

8. **Speakers**

   - **`GET /speakers/search`**, **`POST /speakers/update`**, etc.
   - **Purpose**: Label speakers, merge them, or mark them as hallucinations.
   - **Example**:
     ```bash
     GET /speakers/search?name=John
     POST /speakers/update { "id": 123, "name": "John Doe" }
     ```

9. **`POST /raw_sql`**

   - **Purpose**: Execute raw SQL queries (read-only recommended) against Screenpipe’s SQLite DB.
   - **Body**:
     ```json
     { "query": "SELECT * FROM frames LIMIT 5" }
     ```

10. **Audio/Video Device Lists**

    - **`GET /audio/list`**, **`POST /vision/list`**
    - **Purpose**: If you need to show the user which devices or monitors are available.

11. **Experimental**

    - **`POST /experimental/input_control`** for keyboard/mouse automation.
    - **`/experimental/frames/merge`**, **`/experimental/validate/media`** for special cases.

---

## 4. Short Code Examples

### 4.1 Using the SDK in Client Components

```tsx
"use client";
import { useState, useEffect } from "react";
import { pipe } from "@screenpipe/browser";

export default function MySearchComponent() {
  const [results, setResults] = useState([]);
  const [query, setQuery] = useState("");

  async function doSearch() {
    const resp = await pipe.queryScreenpipe({
      q: query,
      contentType: "ocr",
      limit: 5,
      includeFrames: true,
    });
    if (resp) setResults(resp.data);
  }

  return (
    <div>
      <input value={query} onChange={(e) => setQuery(e.target.value)} />
      <button onClick={doSearch}>Search</button>

      {results.map((item) => (
        <div key={JSON.stringify(item)}>
          {item.type === "OCR" && <p>OCR Text: {item.content.text}</p>}
          {item.type === "Audio" && (
            <p>Transcript: {item.content.transcription}</p>
          )}
        </div>
      ))}
    </div>
  );
}
```

### 4.2 Using the SDK in Server Components / API Routes

```ts
// pages/api/fetch-recordings.ts
import { NextApiRequest, NextApiResponse } from "next";
import { pipe } from "@screenpipe/js";

export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse
) {
  try {
    const data = await pipe.queryScreenpipe({
      appName: "chrome",
      contentType: "audio",
      limit: 10,
    });
    res.status(200).json({ data });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
}
```

### 4.3 Executing Raw SQL on the Screenpipe DB

```ts
// pages/api/frames-sql.ts
import { NextApiRequest, NextApiResponse } from "next";
import { pipe } from "@screenpipe/js";

export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse
) {
  try {
    // Example: retrieve the last 5 frames
    const resp = await fetch("http://localhost:3030/raw_sql", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        query: "SELECT * FROM frames ORDER BY id DESC LIMIT 5",
      }),
    });
    const data = await resp.json();
    res.status(200).json(data);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
}
```

### 4.4 Using AI Inside Your Plugin

```ts
// usage in a server route or server component
import { pipe } from "@screenpipe/js";
import { OpenAI } from "openai";

export async function handler() {
  const settings = await pipe.settings.getAll();
  // the user’s chosen model, or screenpipe-cloud with their purchased credits

  const openai = new OpenAI({
    apiKey:
      settings.aiProviderType === "screenpipe-cloud"
        ? settings?.user?.token
        : settings?.openaiApiKey,
    baseURL: settings?.aiUrl,
  });

  const response = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: [
      { role: "system", content: "You are a helpful assistant." },
      { role: "user", content: "How are you?" },
    ],
  });

  return response.choices[0].message.content;
}
```

---

## 5. Database Schema (Read-Only)

Screenpipe stores all captures in a local SQLite database.  
Use the `/raw_sql` endpoint for read queries. Avoid writing data directly unless absolutely necessary.

**Important Tables**:

- **`frames`**: Screen frames (timestamp, app name, window name).
- **`ocr_text`**: OCR text for each frame.
- **`audio_chunks`**, **`audio_transcriptions`**: Audio data + transcripts.
- **`speakers`** + **`speaker_embeddings`**: For speaker identification.
- **`ui_monitoring`**: UI-scraped text data.

See the provided schema snippet for a full list. Many of these also have associated FTS (full-text search) tables like `ocr_text_fts`, `audio_transcriptions_fts`, etc.

---

## 7. User Settings & Customization

Plugins can let users configure settings (like AI model, custom preferences, etc.). Use the `usePipeSettings` hook in your client UI:

```tsx
"use client";
import { usePipeSettings } from "@/lib/hooks/use-pipe-settings";

export function SettingsPage() {
  const { settings, updateSettings, loading } = usePipeSettings();

  if (loading) return <p>Loading...</p>;

  async function handleSave(e: React.FormEvent<HTMLFormElement>) {
    e.preventDefault();
    const formData = new FormData(e.currentTarget);
    await updateSettings({
      customSetting: formData.get("customSetting") as string,
      anotherSetting: parseInt(formData.get("anotherSetting") as string, 10),
    });
  }

  return (
    <form onSubmit={handleSave}>
      <input name="customSetting" defaultValue={settings?.customSetting} />
      <input
        name="anotherSetting"
        type="number"
        defaultValue={settings?.anotherSetting}
      />
      <button type="submit">Save</button>
    </form>
  );
}
```

Or on the server side:

```ts
import { pipe } from "@screenpipe/js";

async function updateSomething() {
  const currentSettings = await pipe.settings.getAll();
  await pipe.settings.update({ customSetting: "newVal" });
}
```

---

# Screenpipe Database Schema

This schema is the screenpipe database and should be used as readonly via `/raw_sql` endpoint.
It provides the capture data.

```
# Database Schema

## Main Tables

### video_chunks

Stores information about video recording chunks

| Column | Type | Constraints |
|--------|------|-------------|
| id | INTEGER | PRIMARY KEY, AUTOINCREMENT |
| file_path | TEXT | NOT NULL |
| device_name | TEXT | NOT NULL, DEFAULT '' |

### frames

Stores individual frames from video recordings

| Column | Type | Constraints |
|--------|------|-------------|
| id | INTEGER | PRIMARY KEY, AUTOINCREMENT |
| video_chunk_id | INTEGER | NOT NULL, REFERENCES video_chunks(id) |
| offset_index | INTEGER | NOT NULL |
| timestamp | TIMESTAMP | NOT NULL |
| name | TEXT |  |
| browser_url | TEXT | DEFAULT NULL |
| app_name | TEXT | DEFAULT NULL |
| window_name | TEXT | DEFAULT NULL |
| focused | BOOLEAN | DEFAULT NULL |

### audio_chunks

Stores information about audio recording chunks

| Column | Type | Constraints |
|--------|------|-------------|
| id | INTEGER | PRIMARY KEY, AUTOINCREMENT |
| file_path | TEXT | NOT NULL |
| timestamp | TIMESTAMP |  |

### chunked_text_index

Stores unique text chunks for efficient indexing

| Column | Type | Constraints |
|--------|------|-------------|
| text_id | INTEGER | PRIMARY KEY, AUTOINCREMENT |
| text | TEXT | NOT NULL, UNIQUE |

### chunked_text_entries

Maps text chunks to frames or audio chunks

| Column | Type | Constraints |
|--------|------|-------------|
| text_id | INTEGER | NOT NULL, REFERENCES chunked_text_index(text_id) |
| frame_id | INTEGER | REFERENCES frames(id) |
| audio_chunk_id | INTEGER | REFERENCES audio_chunks(id) |
| timestamp | DATETIME | NOT NULL |
| engine | TEXT | NOT NULL |
| chunking_engine | TEXT | NOT NULL |
| source | TEXT | NOT NULL |

### ocr_text

Stores OCR-extracted text from frames

| Column | Type | Constraints |
|--------|------|-------------|
| frame_id | INTEGER | NOT NULL |
| text | TEXT | NOT NULL |
| text_json | TEXT |  |
| app_name | TEXT | NOT NULL, DEFAULT '' |
| ocr_engine | TEXT | NOT NULL, DEFAULT 'unknown' |
| window_name | TEXT |  |
| focused | BOOLEAN | DEFAULT FALSE |
| text_length | INTEGER |  |

### tags

Stores tags for categorizing content

| Column | Type | Constraints |
|--------|------|-------------|
| id | INTEGER | PRIMARY KEY, AUTOINCREMENT |
| name | TEXT | NOT NULL, UNIQUE |
| created_at | DATETIME | DEFAULT CURRENT_TIMESTAMP |

### vision_tags

Maps tags to frames (vision data)

| Column | Type | Constraints |
|--------|------|-------------|
| vision_id | INTEGER | NOT NULL, REFERENCES frames(id) |
| tag_id | INTEGER | NOT NULL, REFERENCES tags(id) |

**Composite Primary Key**: (vision_id, tag_id)

### audio_tags

Maps tags to audio chunks

| Column | Type | Constraints |
|--------|------|-------------|
| audio_chunk_id | INTEGER | NOT NULL, REFERENCES audio_chunks(id) |
| tag_id | INTEGER | NOT NULL, REFERENCES tags(id) |

**Composite Primary Key**: (audio_chunk_id, tag_id)

### ui_monitoring

Stores UI monitoring data

| Column | Type | Constraints |
|--------|------|-------------|
| id | INTEGER | PRIMARY KEY, AUTOINCREMENT |
| text_output | TEXT | NOT NULL |
| timestamp | DATETIME | NOT NULL, DEFAULT CURRENT_TIMESTAMP |
| app | TEXT | NOT NULL |
| window | TEXT | NOT NULL |
| initial_traversal_at | DATETIME |  |
| text_length | INTEGER |  |

### ui_monitoring_tags

Maps tags to UI monitoring entries

| Column | Type | Constraints |
|--------|------|-------------|
| ui_monitoring_id | INTEGER | NOT NULL, REFERENCES ui_monitoring(id) |
| tag_id | INTEGER | NOT NULL, REFERENCES tags(id) |

**Composite Primary Key**: (ui_monitoring_id, tag_id)

### speakers

Stores information about speakers in audio

| Column | Type | Constraints |
|--------|------|-------------|
| id | INTEGER | PRIMARY KEY, AUTOINCREMENT |
| name | TEXT |  |
| metadata | JSON |  |
| hallucination | BOOLEAN | DEFAULT FALSE |

### speaker_embeddings

Stores speaker voice embeddings for identification

| Column | Type | Constraints |
|--------|------|-------------|
| id | INTEGER | PRIMARY KEY, AUTOINCREMENT |
| embedding | FLOAT[512] | NOT NULL, CHECK(typeof(embedding) == 'blob' and vec_length(embedding) == 512) |
| speaker_id | INTEGER | REFERENCES speakers(id) |

### audio_transcriptions

Stores transcriptions of audio chunks

| Column | Type | Constraints |
|--------|------|-------------|
| id | INTEGER | PRIMARY KEY, AUTOINCREMENT |
| audio_chunk_id | INTEGER | NOT NULL, REFERENCES audio_chunks(id) |
| offset_index | INTEGER | NOT NULL |
| timestamp | TIMESTAMP | NOT NULL |
| transcription | TEXT | NOT NULL |
| device | TEXT | NOT NULL, DEFAULT '' |
| is_input_device | BOOLEAN | NOT NULL, DEFAULT TRUE |
| speaker_id | INTEGER |  |
| transcription_engine | TEXT | NOT NULL, DEFAULT 'Whisper' |
| start_time | REAL |  |
| end_time | REAL |  |
| text_length | INTEGER |  |

### ocr_text_embeddings

Stores embeddings for OCR text

| Column | Type | Constraints |
|--------|------|-------------|
| id | INTEGER | PRIMARY KEY, AUTOINCREMENT |
| frame_id | INTEGER | NOT NULL, REFERENCES frames(id) |
| embedding | BLOB | NOT NULL |
| created_at | DATETIME | DEFAULT CURRENT_TIMESTAMP |

### friend_wearable_requests

Stores requests for friend wearable data

| Column | Type | Constraints |
|--------|------|-------------|
| id | INTEGER | PRIMARY KEY, AUTOINCREMENT |
| request_id | TEXT | NOT NULL |
| memory_source | TEXT | NOT NULL |
| chunk_id_range | TEXT | NOT NULL |
| timestamp_range | TEXT | NOT NULL |
| friend_user_id | TEXT | NOT NULL |
| created_at | DATETIME | DEFAULT CURRENT_TIMESTAMP |
| filtered_text | TEXT |  |
| structured_response | TEXT |  |
| response_id | TEXT |  |
| response_created_at | DATETIME |  |
| is_successful | BOOLEAN | NOT NULL, DEFAULT TRUE |

## Full-Text Search Tables

### chunked_text_index_fts

Full-text search table for searching: text

### ocr_text_fts

Full-text search table for searching: text, app_name, window_name

### ui_monitoring_fts

Full-text search table for searching: text_output, app, window

### audio_transcriptions_fts

Full-text search table for searching: transcription, device, speaker_id

### frames_fts

Full-text search table for searching: name, browser_url, app_name, window_name, focused


```

# Screen Pipe API Types

```ts
index.d.ts
---

import { S as Settings, N as NotificationOptions, I as InputAction, a as ScreenpipeQueryParams, b as ScreenpipeResponse } from './types-DlwJxlZs.js';
export { j as AIProviderType, i as ActionResponse, A as AudioContent, d as ContentItem, C as ContentType, E as EmbeddedLLMConfig, p as EventStreamResponse, g as InboxMessage, h as InboxMessageAction, e as InputControlResponse, f as NotificationAction, O as OCRContent, P as PaginationInfo, m as ParsedConfig, l as PipeConfig, c as Speaker, T as TranscriptionChunk, n as TranscriptionStreamResponse, U as UiContent, k as User, V as VisionEvent, o as VisionStreamResponse } from './types-DlwJxlZs.js';
import { SettingsManager } from './SettingsManager.js';
import { InboxManager } from './InboxManager.js';

type Result<T> = {
    success: true;
    data: T;
} | {
    success: false;
    error: any;
};
declare class PipesManager {
    list(): Promise<Result<string[]>>;
    download(url: string): Promise<Result<Record<string, any>>>;
    enable(pipeId: string): Promise<boolean>;
    disable(pipeId: string): Promise<boolean>;
    update(pipeId: string, config: {
        [key: string]: string;
    }): Promise<boolean>;
    info(pipeId: string): Promise<Result<Record<string, any>>>;
    downloadPrivate(url: string, pipeName: string, pipeId: string): Promise<Result<Record<string, any>>>;
    delete(pipeId: string): Promise<boolean>;
}

declare function getDefaultSettings(): Settings;

declare class NodePipe {
    private analyticsInitialized;
    private analyticsEnabled;
    input: {
        type: (text: string) => Promise<boolean>;
        press: (key: string) => Promise<boolean>;
        moveMouse: (x: number, y: number) => Promise<boolean>;
        click: (button: "left" | "right" | "middle") => Promise<boolean>;
    };
    settings: SettingsManager;
    inbox: InboxManager;
    pipes: PipesManager;
    sendDesktopNotification(options: NotificationOptions): Promise<boolean>;
    sendInputControl(action: InputAction): Promise<boolean>;
    /**
     * Query Screenpipe for content based on various filters.
     *
     * @param params - Query parameters for filtering Screenpipe content
     * @returns Promise resolving to the Screenpipe response or null
     *
     * @example
     * // Basic search for recent browser activity on a specific website
     * const githubActivity = await pipe.queryScreenpipe({
     *   browserUrl: "github.com",
     *   contentType: "ocr",
     *   limit: 20,
     *   includeFrames: true
     * });
     *
     * @example
     * // Search for specific text on a particular website with date filters
     * const searchResults = await pipe.queryScreenpipe({
     *   q: "authentication",
     *   browserUrl: "auth0.com",
     *   appName: "Chrome",
     *   contentType: "ocr",
     *   startTime: new Date(Date.now() - 7 * 24 * 60 * 60 * 1000).toISOString(),
     *   endTime: new Date().toISOString(),
     *   limit: 50
     * });
     *
     * @example
     * // Track history of visits to a specific web application
     * type VisitSession = {
     *   timestamp: string;
     *   title: string;
     *   textContent: string;
     *   imageData?: string;
     * };
     *
     * async function getAppUsageHistory(domain: string): Promise<VisitSession[]> {
     *   try {
     *     const results = await pipe.queryScreenpipe({
     *       browserUrl: domain,
     *       contentType: "ocr",
     *       includeFrames: true,
     *       limit: 100,
     *       startTime: new Date(Date.now() - 30 * 24 * 60 * 60 * 1000).toISOString() // last 30 days
     *     });
     *
     *     return results.data
     *       .filter(item => item.type === "OCR")
     *       .map(item => {
     *         const ocrItem = item.content as OCRContent;
     *         return {
     *           timestamp: ocrItem.timestamp,
     *           title: ocrItem.windowName || '',
     *           textContent: ocrItem.text,
     *           imageData: ocrItem.frame
     *         };
     *       });
     *   } catch (error) {
     *     console.error("Failed to retrieve app usage history:", error);
     *     return [];
     *   }
     * }
     *
     * @example
     * // Combining browserUrl with speaker filters for meeting recordings in browser
     * import { pipe, ContentType, ScreenpipeResponse } from '@screenpipe/js';
     *
     * interface MeetingData {
     *   url: string;
     *   speakerName: string;
     *   transcript: string;
     *   timestamp: string;
     * }
     *
     * async function getMeetingTranscripts(
     *   meetingUrl: string,
     *   speakerIds: number[]
     * ): Promise<MeetingData[]> {
     *   try {
     *     const results = await pipe.queryScreenpipe({
     *       browserUrl: meetingUrl,
     *       contentType: "audio" as ContentType,
     *       speakerIds: speakerIds,
     *       limit: 200
     *     });
     *
     *     return results.data
     *       .filter(item => item.type === "Audio")
     *       .map(item => {
     *         const audioItem = item.content;
     *         return {
     *           url: meetingUrl,
     *           speakerName: audioItem.speaker?.name || 'Unknown',
     *           transcript: audioItem.transcription,
     *           timestamp: audioItem.timestamp
     *         };
     *       });
     *   } catch (error) {
     *     console.error(`Error fetching meeting transcripts for ${meetingUrl}:`, error);
     *     return [];
     *   }
     * }
     */
    queryScreenpipe(params: ScreenpipeQueryParams): Promise<ScreenpipeResponse | null>;
    private initAnalyticsIfNeeded;
    captureEvent(eventName: string, properties?: Record<string, any>): Promise<void>;
    captureMainFeatureEvent(featureName: string, properties?: Record<string, any>): Promise<void>;
}
declare const pipe: NodePipe;

export { InputAction, NotificationOptions, ScreenpipeQueryParams, ScreenpipeResponse, Settings, getDefaultSettings, pipe };

-- types.dt.s
/**
 * Types of content that can be queried in Screenpipe.
 */
type ContentType = "all" | "ocr" | "audio" | "ui" | "audio+ui" | "ocr+ui" | "audio+ocr";
/**
 * Parameters for querying Screenpipe.
 */
interface ScreenpipeQueryParams {
    /** Optional search query text */
    q?: string;
    /** Type of content to search for (default: "all") */
    contentType?: ContentType;
    /** Maximum number of results to return (default: 10) */
    limit?: number;
    /** Number of results to skip (for pagination) */
    offset?: number;
    /** Filter results after this ISO timestamp (e.g. "2023-01-01T00:00:00Z") */
    startTime?: string;
    /** Filter results before this ISO timestamp (e.g. "2023-01-01T00:00:00Z") */
    endTime?: string;
    /** Filter by application name (e.g. "chrome", "vscode") */
    appName?: string;
    /** Filter by window title */
    windowName?: string;
    /** Include base64-encoded screenshot frames in results */
    includeFrames?: boolean;
    /** Filter by minimum text length */
    minLength?: number;
    /** Filter by maximum text length */
    maxLength?: number;
    /** Filter by specific speaker IDs */
    speakerIds?: number[];
    /** Filter by frame name */
    frameName?: string;
    /** Filter by browser URL (for web content) */
    browserUrl?: string;
}
/**
 * Structure of OCR (Optical Character Recognition) content.
 */
interface OCRContent {
    frameId: number;
    text: string;
    timestamp: string;
    filePath: string;
    offsetIndex: number;
    appName: string;
    windowName: string;
    tags: string[];
    frame?: string;
    frameName?: string;
    browserUrl?: string;
    focused?: boolean;
}
/**
 * Structure of audio content.
 */
interface AudioContent {
    chunkId: number;
    transcription: string;
    timestamp: string;
    filePath: string;
    offsetIndex: number;
    tags: string[];
    deviceName: string;
    deviceType: string;
    speaker?: Speaker;
    startTime?: number;
    endTime?: number;
}
/**
 * Structure of UI content.
 */
interface UiContent {
    id: number;
    text: string;
    timestamp: string;
    appName: string;
    windowName: string;
    initialTraversalAt?: string;
    filePath: string;
    offsetIndex: number;
    frameName?: string;
    browserUrl?: string;
}
/**
 * Speaker information
 */
interface Speaker {
    id: number;
    name?: string;
    metadata?: string;
}
/**
 * Union type for different types of content items.
 */
type ContentItem = {
    type: "OCR";
    content: OCRContent;
} | {
    type: "Audio";
    content: AudioContent;
} | {
    type: "UI";
    content: UiContent;
};
/**
 * Pagination information for search results.
 */
interface PaginationInfo {
    limit: number;
    offset: number;
    total: number;
}
/**
 * Structure of the response from a Screenpipe query.
 */
interface ScreenpipeResponse {
    data: ContentItem[];
    pagination: PaginationInfo;
}
/**
 * Input control action types
 */
type InputAction = {
    type: "WriteText";
    data: string;
} | {
    type: "KeyPress";
    data: string;
} | {
    type: "MouseMove";
    data: {
        x: number;
        y: number;
    };
} | {
    type: "MouseClick";
    data: "left" | "right" | "middle";
};
/**
 * Response from input control operations
 */
interface InputControlResponse {
    success: boolean;
}
/**
 * Notification options
 */
interface NotificationOptions {
    title: string;
    body: string;
    actions?: NotificationAction[];
    timeout?: number;
    persistent?: boolean;
}
interface NotificationAction {
    id: string;
    label: string;
    callback?: () => Promise<void>;
}
/**
 * Inbox message structure
 */
interface InboxMessage {
    title: string;
    body: string;
    actions?: InboxMessageAction[];
}
interface InboxMessageAction {
    label: string;
    action: string;
    callback: () => Promise<void>;
}
interface ActionResponse {
    action: string;
}
/**
 * Settings types
 */
type AIProviderType = "native-ollama" | "openai" | "custom" | "embedded" | "screenpipe-cloud";
interface EmbeddedLLMConfig {
    enabled: boolean;
    model: string;
    port: number;
}
interface User {
    id?: string;
    email?: string;
    name?: string;
    image?: string;
    token?: string;
    clerk_id?: string;
    credits?: {
        amount: number;
    };
}
interface Settings {
    openaiApiKey: string;
    deepgramApiKey: string;
    aiModel: string;
    aiUrl: string;
    customPrompt: string;
    port: number;
    dataDir: string;
    disableAudio: boolean;
    ignoredWindows: string[];
    includedWindows: string[];
    aiProviderType: AIProviderType;
    embeddedLLM: EmbeddedLLMConfig;
    enableFrameCache: boolean;
    enableUiMonitoring: boolean;
    aiMaxContextChars: number;
    analyticsEnabled: boolean;
    user: User;
    customSettings?: Record<string, any>;
    monitorIds: string[];
    audioDevices: string[];
    audioTranscriptionEngine: string;
    enableRealtimeAudioTranscription: boolean;
    realtimeAudioTranscriptionEngine: string;
    disableVision: boolean;
}
/**
 * Pipe configuration types
 */
interface PipeConfig {
    [key: string]: any;
}
interface ParsedConfig<T = unknown> {
    fields: {
        name: string;
        value?: T;
        default?: T;
    }[];
}
interface TranscriptionChunk {
    transcription: string;
    timestamp: string;
    device: string;
    is_input: boolean;
    is_final: boolean;
    speaker?: string;
}
interface TranscriptionStreamResponse {
    id: string;
    object: string;
    created: number;
    model: string;
    choices: Array<{
        text: string;
        index: number;
        finish_reason: string | null;
    }>;
    metadata?: {
        timestamp: string;
        device: string;
        isInput: boolean;
        speaker?: string;
    };
}
interface VisionEvent {
    image?: string;
    text: string;
    timestamp: string;
    app_name?: string;
    window_name?: string;
    browser_url?: string;
}
interface VisionStreamResponse {
    type: string;
    data: VisionEvent;
}
interface EventStreamResponse {
    name: string;
    data: VisionEvent | TranscriptionChunk | any;
}

export type { AudioContent as A, ContentType as C, EmbeddedLLMConfig as E, InputAction as I, NotificationOptions as N, OCRContent as O, PaginationInfo as P, Settings as S, TranscriptionChunk as T, UiContent as U, VisionEvent as V, ScreenpipeQueryParams as a, ScreenpipeResponse as b, Speaker as c, ContentItem as d, InputControlResponse as e, NotificationAction as f, InboxMessage as g, InboxMessageAction as h, ActionResponse as i, AIProviderType as j, User as k, PipeConfig as l, ParsedConfig as m, TranscriptionStreamResponse as n, VisionStreamResponse as o, EventStreamResponse as p };

-- InboxManager.d.ts

import { g as InboxMessage } from './types-DlwJxlZs.js';

declare class InboxManager {
    private actionServerPort?;
    private actionServerProcess?;
    send(message: InboxMessage): Promise<boolean>;
}

export { InboxManager };

-- SettingsManager.d.ts

import { S as Settings } from './types-DlwJxlZs.js';

declare class SettingsManager {
    private settings;
    private storePath;
    private initialized;
    constructor();
    private getStorePath;
    init(): Promise<void>;
    save(): Promise<void>;
    get<K extends keyof Settings>(key: K): Promise<Settings[K]>;
    set<K extends keyof Settings>(key: K, value: Settings[K]): Promise<void>;
    getAll(): Promise<Settings>;
    update(newSettings: Partial<Settings>): Promise<void>;
    reset(): Promise<void>;
    resetKey<K extends keyof Settings>(key: K): Promise<void>;
    getCustomSetting(namespace: string, key: string): Promise<any>;
    setCustomSetting(namespace: string, key: string, value: any): Promise<void>;
    getNamespaceSettings(namespace: string): Promise<Record<string, any> | undefined>;
    updateNamespaceSettings(namespace: string, settings: Record<string, any>): Promise<void>;
}

export { SettingsManager };

```

## 8. Final Notes

- **Security**: Data is unencrypted by default but remains local.
- **Performance**: Search or indexing over large data sets can be resource-intensive.
- **No migrations** required for Screenpipe’s DB. You can manage your plugin’s own DB as you see fit.
- Keep your code well-structured. Build exactly what’s requested, with full implementations.
- You can reference Screenpipe’s database schema (read-only) via the `POST /raw_sql` route if needed. The schema includes tables like `frames`, `ocr_text`, `audio_chunks`, `audio_transcriptions`, etc.
- For advanced tasks like controlling keyboard/mouse, see the `/experimental/input_control` endpoint (node context only).

# Chat moderation

Example usage of openai for chat moderation.

```js
import OpenAI from "openai";
const openai = new OpenAI();

const moderation = await openai.moderations.create({
  model: "omni-moderation-latest",
  input: "...text to classify goes here...",
});

console.log(moderation);
```

# Partykit for Chat Server / Client

The party-server directory provides the chat server and leaderboard management.

The lib/party-kit/party-kit-client.ts is for a front-end component to integrate chat and leaderboard functionality.

# Classifying Screen data

### 1. Strategy for Distilling Screen Data (Client-Side)

**Goal**: Minimize the raw data sent to the LLM while still providing enough context to judge “productive vs unproductive.”

**Approach**:

1. **Capture**: Collect raw screen data (active app/window title, partial screen text, timestamps).
2. **Chunking**: Break text into small chunks (~200–300 tokens) to avoid sending huge contexts.
   - Example: If the user is browsing a dev site or social media, chunk it by paragraphs or by time intervals.
3. **Feature Extraction** (before sending to LLM):
   - Truncate or hash any sensitive user content if it’s not strictly needed.
   - Keep only relevant phrases (e.g., “stack overflow,” “youtube,” “github.com,” “facebook,” etc.).
4. **Prompt Construction**: Send each chunk with a short system or user prompt clarifying the user’s goal. For example:

   ```txt
   System Prompt:
   "You classify user activity. Determine if it's 'productive' or 'unproductive' for the user's goal: software development.
    Output only 'productive' or 'unproductive'."
   User Prompt:
   "Window Title: 'Chrome - StackOverflow: Python'
    Partial Text: 'How do I parse JSON in Python? ...'
    Timestamp: 2025-03-01 10:15"
   ```

5. **Post-Processing**: On client side, aggregate the classification results from each chunk into a single 15-minute block rating.
   - If the LLM mostly says “productive,” label that block productive.
   - Possibly include a confidence mechanism (e.g., “productive” 4 out of 5 times => block is productive).

This ensures you’re sending only partial, anonymized data (the “distilled” info) and not entire screen captures. The final classification (true/false) can be aggregated client-side before you send the result to your backend.

---

### 3. Strategy for an LLM Productivity Classification

Since you want a consistent approach for everyone, define clear classification rules and keep the prompt consistent:

6. **System Prompt**: Outline your definition of productivity. For example:

   ```txt
   "You are an assistant that classifies user screen usage.
    • Productive if it aligns with user’s stated goal (e.g., coding, reading documentation).
    • Unproductive if it's purely social media, entertainment, or unrelated browsing.
    Output only the word: 'productive' or 'unproductive'."
   ```

7. **Additional Context**: Let the user specify their role/goal once (e.g., “I’m a web developer, so YouTube is unproductive unless it’s a coding tutorial…”).
8. **Parsing the Return**: The client just checks if the response is `"productive"` or `"unproductive"` and increments/decrements the user’s score.

This approach is fair for everyone because it uses one standard set of guidelines. You can refine the rules over time if you see users gaming the system.

**Handling Breaks**  
A common approach is to classify “break time” as its own category (neither productive nor unproductive). You can detect it either automatically (no keyboard/mouse input for a certain threshold) or let the user manually toggle “break mode.”

1. **Automatic Detection**

   - If no activity (no clicks, keypresses, or app changes) for, say, 5+ minutes, mark that period as “break.”
   - Store it as its own category in your logs.
   - Skip it in productivity score calculations (or count it as neutral).

2. **Manual Break Mode**

   - The user can toggle “On Break” in your UI.
   - Stop tracking or classifying screen usage in that mode.
   - When “Break” ends, resume classification.

3. **Partial Blocks**

   - If a 15-minute block is half break, half active, you can split it or simply label the block by majority status.

---

**Turning the Program Off/On**

- If the user shuts down or quits the app, treat that gap like a break or “no data.”
- When they restart, just log the new active blocks normally.
- No major issues—just make sure your data pipeline expects missing chunks and doesn’t penalize a user for them (like adding negative or zero by default).
